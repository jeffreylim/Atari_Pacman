{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e75674",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "\n",
    "# DQN\n",
    "- ## MsPacman\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee5e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import dataclasses\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "import baselines.common.atari_wrappers as baselines\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee963356",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46c62df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.15.7\n",
      "tensorflow: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(f'{gym.__name__}: {gym.__version__}')\n",
    "print(f'{tf.__name__}: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d428f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
    "\n",
    "# make_atari enables NoopResetEnv and MaxAndSkipEnv\n",
    "env = baselines.make_atari(ENV_NAME)\n",
    "\n",
    "SEED = 22\n",
    "env.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43537ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = baselines.wrap_deepmind(env,\n",
    "                            episode_life=True,\n",
    "                            clip_rewards=True,\n",
    "                            frame_stack=True,\n",
    "                            scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5735de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additionalImageCropping(obs):\n",
    "    return np.array(obs)[10:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969f1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, filename=f'{ENV_NAME}.log', filemode='a', level=logging.INFO, datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2855a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Linear Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f2ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class LinearDecayFactor:\n",
    "    \n",
    "    max_steps: int = 50_000  #1_000_000\n",
    "    epsilon_max: float = 1.0\n",
    "    epsilon_min: float = 0.1\n",
    "\n",
    "    decay_factor = (epsilon_max - epsilon_min) / (max_steps)\n",
    "\n",
    "    def __call__(self, current_step_count: int) -> float:\n",
    "        self.epsilon_decay = self.epsilon_max - self.decay_factor * current_step_count\n",
    "        return max(self.epsilon_min, self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5ea4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Replay Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6800ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayExperiences:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.capacity = 20_000  #100_000\n",
    "        self.batch_size = 32\n",
    "        self.idx = 0\n",
    "        self.num_experiences = 0\n",
    "\n",
    "        self.actions = np.empty(self.capacity, dtype=np.int32)\n",
    "        self.obss = np.empty(self.capacity, dtype=np.ndarray)\n",
    "        self.next_obss = np.empty(self.capacity, dtype=np.ndarray)\n",
    "        self.rewards = np.empty(self.capacity, dtype=np.float32)\n",
    "        self.dones = np.empty(self.capacity, dtype=np.float32)\n",
    "\n",
    "    def append(self, action, obs, next_obs, reward, done):\n",
    "        self.actions[self.idx] = action\n",
    "        self.obss[self.idx] = obs\n",
    "        self.next_obss[self.idx] = next_obs\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.dones[self.idx] = done\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.num_experiences = min(self.capacity, self.num_experiences + 1)\n",
    "        \n",
    "    def sample(self, sampling_with_replacement=False):\n",
    "        indices = np.random.choice(self.num_experiences, size=self.batch_size, replace=sampling_with_replacement)\n",
    "        actions = np.array([self.actions[i] for i in indices])\n",
    "        obss = np.array([self.obss[i] for i in indices])\n",
    "        next_obss = np.array([self.next_obss[i] for i in indices])\n",
    "        rewards = np.array([self.rewards[i] for i in indices])\n",
    "        dones = np.array([self.dones[i] for i in indices])\n",
    "        return actions, obss, next_obss, rewards, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c9ffa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DoubleDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5da86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doubledqn(learning_rate, action_space_dim):\n",
    "    \n",
    "    dqn = tf.keras.Sequential(name='double_dqn')\n",
    "    dqn.add(tf.keras.Input(shape=(84, 84, 4)))\n",
    "    dqn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=[8, 8], strides=[4, 4], activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0)))\n",
    "    dqn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=[4, 4], strides=[2, 2], activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0)))\n",
    "    dqn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0)))\n",
    "    dqn.add(tf.keras.layers.Flatten())\n",
    "    dqn.add(tf.keras.layers.Dense(units=512, activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0)))\n",
    "    dqn.add(tf.keras.layers.Dense(units=action_space_dim, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0)))\n",
    "    #print(dqn.summary())\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, clipnorm=1.0)\n",
    "    dqn.compile(optimizer=optimizer, loss=tf.keras.losses.Huber())\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341b03b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DoubleDuelDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581cff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doubledueldqn(learning_rate, action_space_dim):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(84, 84, 4), name='Input_layer')\n",
    "\n",
    "    layer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=[8, 8], strides=[4, 4], activation='relu')(inputs)\n",
    "    layer2 = tf.keras.layers.Conv2D(filters=64, kernel_size=[4, 4], strides=[2, 2], activation='relu')(layer1)\n",
    "    layer3 = tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], activation='relu')(layer2)\n",
    "    layer4 = tf.keras.layers.Flatten()(layer3)\n",
    "    layer5 = tf.keras.layers.Dense(units=512, activation='relu')(layer4)\n",
    "\n",
    "    action = tf.keras.layers.Dense(units=action_space_dim, activation='linear')(layer5)\n",
    "\n",
    "    state_values = tf.keras.layers.Dense(1)(layer5)\n",
    "    raw_advantages = tf.keras.layers.Dense(action_space_dim)(layer5)\n",
    "    advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1, keepdims=True)\n",
    "    Q_values = state_values + advantages\n",
    "\n",
    "    dqn = tf.keras.Model(inputs=inputs, outputs=[Q_values], name='double_duel_dqn')\n",
    "    #print(dqn.summary())\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, clipnorm=1.0)\n",
    "    dqn.compile(optimizer=optimizer, loss=tf.keras.losses.Huber())\n",
    "        \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2efe67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb88878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, model_name, retrain_model=False):\n",
    "    \n",
    "    discount_factor_gamma = 0.99\n",
    "\n",
    "    update_main_model_freq = 4\n",
    "    update_target_model_freq = 5000  #10_000\n",
    "\n",
    "    if not retrain_model:\n",
    "        current_frame, episode_count, best_episode_reward = 0, 0, 0\n",
    "        memories = ReplayExperiences()\n",
    "        main_dqn = make_doubledqn(learning_rate=0.0001, action_space_dim=env.action_space.n)\n",
    "        target_dqn = make_doubledqn(learning_rate=0.0001, action_space_dim=env.action_space.n)\n",
    "        target_dqn.set_weights(main_dqn.get_weights())\n",
    "    else:\n",
    "        with open('memories.pkl', 'rb') as replay, \\\n",
    "             open('current_frame.pkl', 'rb') as decay, \\\n",
    "             open('episode_count.pkl', 'rb') as episode, \\\n",
    "             open('best_reward.pkl', 'rb') as best_run:\n",
    "                memories = pickle.load(replay)\n",
    "                current_frame = pickle.load(decay)\n",
    "                episode_count = pickle.load(episode)\n",
    "                best_episode_reward = pickle.load(best_run)\n",
    "                logging.info(f'Loading current_frame: {current_frame}')\n",
    "                logging.info(f'Loading episode_count: {episode_count}')\n",
    "                logging.info(f'Loading best_episode_reward: {best_episode_reward}')\n",
    "                logging.info(f'Memories buffer length: {memories.num_experiences}')\n",
    "        \n",
    "        main_dqn = tf.keras.models.load_model(f'{model_name}_main.h5')\n",
    "        target_dqn = tf.keras.models.load_model(f'{model_name}_target.h5')\n",
    "\n",
    "        \n",
    "    max_frames = 5 * 1e7\n",
    "    max_episode_length = 18_000\n",
    "\n",
    "    epsilon_decay = LinearDecayFactor()\n",
    "\n",
    "    while current_frame < max_frames:\n",
    "\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for _ in range(max_episode_length):\n",
    "\n",
    "            current_frame += 1\n",
    "\n",
    "            if epsilon_decay(current_frame) > np.random.uniform():\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                q_values = main_dqn(obs[np.newaxis], training=False)\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            memories.append(action, obs, next_obs, reward, done)\n",
    "            episode_reward += reward\n",
    "            obs = next_obs\n",
    "\n",
    "            if current_frame % update_main_model_freq == 0 and current_frame > memories.batch_size:\n",
    "                \n",
    "                sampled_actions, sampled_obs, sampled_next_obs, sampled_rewards, sampled_dones = memories.sample()\n",
    "                \n",
    "                next_q_values_model = main_dqn.predict(sampled_next_obs, verbose=0)\n",
    "                best_actions_next_q_values_model = next_q_values_model.argmax(axis=1)\n",
    "\n",
    "                next_action_mask = tf.one_hot(best_actions_next_q_values_model, env.action_space.n).numpy()\n",
    "                next_q_values_target = target_dqn.predict(sampled_next_obs, verbose=0)\n",
    "                next_q_values_target = tf.reduce_sum(next_q_values_target * next_action_mask, axis=1).numpy()\n",
    "                next_targeted_q_values = sampled_rewards + discount_factor_gamma * next_q_values_target * (1 - sampled_dones)\n",
    "                next_targeted_q_values = next_targeted_q_values.reshape(-1, 1)\n",
    "\n",
    "                current_action_mask = tf.one_hot(sampled_actions, env.action_space.n)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    current_q_values = main_dqn(sampled_obs)\n",
    "                    current_q_values = tf.reduce_sum(current_q_values * current_action_mask, axis=1, keepdims=True)\n",
    "                    loss = tf.keras.losses.Huber()(next_targeted_q_values, current_q_values)\n",
    "\n",
    "                grads = tape.gradient(loss, main_dqn.trainable_variables)\n",
    "                main_dqn.optimizer.apply_gradients(zip(grads, main_dqn.trainable_variables))\n",
    "\n",
    "            if current_frame % update_target_model_freq == 0 and current_frame > 2000:\n",
    "                target_dqn.set_weights(main_dqn.get_weights())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_count += 1\n",
    "        logging.info(f'Episode: {episode_count}, Reward: {episode_reward}, Frame#: {current_frame}')\n",
    "\n",
    "        if episode_reward > best_episode_reward:\n",
    "            best_episode_reward = episode_reward\n",
    "            main_dqn.save(f'{model_name}_main_{episode_reward}.h5')\n",
    "            target_dqn.save(f'{model_name}_target_{episode_reward}.h5')\n",
    "            logging.info(f'Saving main and target models at episode: {episode_count}, Reward: {episode_reward}')\n",
    "    \n",
    "        if episode_count % 300000 == 0:\n",
    "            logging.info(f'Dumping model at episode_count: {episode_count}, current_frame: {current_frame}')\n",
    "            main_dqn.save(f'{model_name}_main.h5')\n",
    "            target_dqn.save(f'{model_name}_target.h5')\n",
    "            with open('memories.pkl', 'wb') as replay, \\\n",
    "                 open('current_frame.pkl', 'wb') as decay, \\\n",
    "                 open('episode_count.pkl', 'wb') as episode, \\\n",
    "                 open('best_reward.pkl', 'wb') as best_run:\n",
    "                    pickle.dump(memories, replay)\n",
    "                    pickle.dump(current_frame, decay)\n",
    "                    pickle.dump(episode_count, episode)\n",
    "                    pickle.dump(best_episode_reward, best_run)\n",
    "            #break\n",
    "\n",
    "    main_dqn.save(f'{model_name}_main.h5')\n",
    "    target_dqn.save(f'{model_name}_target.h5')\n",
    "    logging.info(f'Saving completed main and target models at episode: {episode_count}, Reward: {episode_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e460796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    train(env, ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e8a06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5405d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, eval_model, output_dir, num_of_evals=10):\n",
    "\n",
    "    env = gym.wrappers.Monitor(env, output_dir, force=True)\n",
    "\n",
    "    for _ in range(num_of_evals):\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_reward, step_count = 0, 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            obs_t = tf.convert_to_tensor(obs)\n",
    "            obs_t = tf.expand_dims(obs_t, 0)\n",
    "\n",
    "            action_values = trained_model.predict(obs_t, verbose=0)\n",
    "            action = tf.argmax(action_values[0]).numpy()\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a28be733",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model_name = f'{ENV_NAME}_main.h5'\n",
    "    trained_model = tf.keras.models.load_model(model_name)\n",
    "    \n",
    "    output_dir = f'./Evaluation_{datetime.datetime.now().strftime(\"%d%m%Y\")}'\n",
    "    evaluate(env, trained_model, output_dir)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571fbe23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff1e7c",
   "metadata": {},
   "source": [
    "### Starting out $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a2c43",
   "metadata": {},
   "source": [
    "<img src='utils/pacman_starting_out.gif' height='420' width='320'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8109d75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### On the way $\\ldots$\n",
    "\n",
    "<img src='utils/pacman_on_the_way.gif' height='420' width='320'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7195e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Still on the way $\\ldots$\n",
    "\n",
    "<img src='utils/pacman_still_on_way1.gif' height='420' width='320'/>\n",
    "\n",
    "<img src='utils/pacman_still_on_way2.gif' height='420' width='320'/>\n",
    "\n",
    "<img src='utils/pacman_still_on_way3.gif' height='420' width='320'/>\n",
    "\n",
    "<img src='utils/pacman_still_on_way4.gif' height='420' width='320'/>\n",
    "\n",
    "<img src='utils/pacman_still_on_way5.gif' height='420' width='320'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
